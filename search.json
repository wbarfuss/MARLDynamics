[
  {
    "objectID": "Utilities/uflowplot.html",
    "href": "Utilities/uflowplot.html",
    "title": "FlowPlot",
    "section": "",
    "text": "source\n\n\n\n plot_strategy_flow (mae, x:tuple, y:tuple, flowarrow_points,\n                     NrRandom:int=3, use_RPEarrows=True, col:str='LEN',\n                     cmap='viridis', kind='quiver+samples', sf=0.5,\n                     lw=1.0, dens=0.75, acts=None, conds=None,\n                     axes:Iterable=None, verbose=False)\n\nCreate a flow plot in strategy space.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmae\n\n\nMARLDynamics multi-agent environment object\n\n\nx\ntuple\n\nwhich phase space axes to plot along x axes\n\n\ny\ntuple\n\nwhich phase space axes to plot along y axes\n\n\nflowarrow_points\n\n\nspecify range & resolution of flow arrows\n\n\nNrRandom\nint\n3\nhow many random (in the other dimensions) stratgies for averaging\n\n\nuse_RPEarrows\nbool\nTrue\nUse reward-prediction error arrows?, otherwise use strategy differences\n\n\ncol\nstr\nLEN\ncolor indicates either strength of flow via colormap, otherwise a fixed color name\n\n\ncmap\nstr\nviridis\nColormap\n\n\nkind\nstr\nquiver+samples\nKind of plot: “streamplot”, “quiver+samples”, “quiver”, …\n\n\nsf\nfloat\n0.5\nScale factor for quiver arrows\n\n\nlw\nfloat\n1.0\nLine width for streamplot\n\n\ndens\nfloat\n0.75\nDensity for streamplot\n\n\nacts\nNoneType\nNone\nAction descriptions\n\n\nconds\nNoneType\nNone\nConditions descriptions\n\n\naxes\ntyping.Iterable\nNone\nAxes to plot into\n\n\nverbose\nbool\nFalse\nshall I talk to you while working?\n\n\n\n\nsource\n\n\n\n\n plot_trajectories (Xtrajs:Iterable, x:tuple, y:tuple,\n                    cols:Iterable=['r'], alphas:Iterable=[1.0],\n                    lss:Iterable=['-'], lws:Iterable=[2],\n                    mss:Iterable=[None], msss:Iterable=[0],\n                    fprs:Union[Iterable,bool]=None, axes:Iterable=None,\n                    submean=False)\n\nPlot multiple trajectories in phase space.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nXtrajs\ntyping.Iterable\n\nIterable of phase space trajectories\n\n\nx\ntuple\n\nwhich phase space axes to plot along x axes\n\n\ny\ntuple\n\nwhich phase space axes to plot along y axes\n\n\ncols\ntyping.Iterable\n[‘r’]\nColors to iterate through\n\n\nalphas\ntyping.Iterable\n[1.0]\nAlpha values to iterate through\n\n\nlss\ntyping.Iterable\n[‘-’]\nLinestyles to iterate through\n\n\nlws\ntyping.Iterable\n[2]\nLinewidths to iterate through\n\n\nmss\ntyping.Iterable\n[None]\nMarkers to iterate through\n\n\nmsss\ntyping.Iterable\n[0]\nMarker sizes to iterate through\n\n\nfprs\ntyping.Union[typing.Iterable, bool]\nNone\nIteralbe indicating which trajectories reached a fixed point\n\n\naxes\ntyping.Iterable\nNone\nAxes to plot into\n\n\nsubmean\nbool\nFalse"
  },
  {
    "objectID": "Utilities/uflowplot.html#helpers",
    "href": "Utilities/uflowplot.html#helpers",
    "title": "FlowPlot",
    "section": "Helpers",
    "text": "Helpers\n\nsource\n\n_checks_and_balances\n\n _checks_and_balances (x:tuple, y:tuple)\n\nCheck the format of the x and y parameter.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nx\ntuple\nwhich phase space axes to plot along x axes\n\n\ny\ntuple\nwhich phase space axes to plot along y axes\n\n\nReturns\ntuple\n(lengths for each dimension, index of dimension to iter, length of iter)\n\n\n\nLet’s say we want to plot the probability of cooperation of the 0th agent on the \\(x\\) axis and of the 1st agent on the \\(y\\) axis for states 2,3 and 5, we specify (assuming the cooperation is the 0th action)\n\nx = ([0], [2,3,5], [0])\ny = ([1], [2,3,5], [0])\n_checks_and_balances(x, y)\n\n(array([1, 3, 1]), 1, 3)\n\n\n\nsource\n\n\n_prepare_axes\n\n _prepare_axes (axes:Iterable, xlens:tuple)\n\nCheck whether axes have been provided correctly. If axes haven’t been provided, provide them.\n\n\n\n\nType\nDetails\n\n\n\n\naxes\ntyping.Iterable\nAxes to plot into\n\n\nxlens\ntuple\nLengths for each dimension of x and y\n\n\nReturns\ntyping.Iterable\nof matplotlib axes\n\n\n\n\n_prepare_axes(None, [1,3,1])\n\narray([<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>], dtype=object)\n\n\n\n\n\n\nsource\n\n\n_dXisa_s\n\n _dXisa_s (Xisa_s:Iterable, mae)\n\nCompute Xisa(t-1)-Xisa(t) for all Xisa_s.\n\n\n\n\nType\nDetails\n\n\n\n\nXisa_s\ntyping.Iterable\nof joint strategies Xisa\n\n\nmae\n\nMARLDynamics multi-agent environment object\n\n\nReturns\nndarray\njoint strategy differences\n\n\n\n\nfrom MARLDynamics.Agents.StrategyActorCritic import stratAC\nfrom MARLDynamics.Environments.SocialDilemma import SocialDilemma\n\n\nenv = SocialDilemma(R=1.0, T=0.8, S=-0.5, P=0.0)\nmae = stratAC(env=env, learning_rates=0.1, discount_factors=0.9)\n\nXisa_s = [mae.random_softmax_strategy() for _ in range(7)]\n_dXisa_s(Xisa_s, mae).shape\n\n(7, 2, 1, 2)\n\n\n\nsource\n\n\n_dRPEisa_s\n\n _dRPEisa_s (Xisa_s:Iterable, mae)\n\nCompute reward-prediction errors RPEisa_s for Xs.\n\n\n\n\nType\nDetails\n\n\n\n\nXisa_s\ntyping.Iterable\nof joint strategies Xisa\n\n\nmae\n\nMARLDynamics multi-agent environment object\n\n\nReturns\nndarray\njoint reward-prediction errors\n\n\n\n\nenv = SocialDilemma(R=1.0, T=0.8, S=-0.5, P=0.0)\nmae = stratAC(env=env, learning_rates=0.1, discount_factors=0.9)\n\nXisa_s = [mae.random_softmax_strategy() for _ in range(7)]\n_dRPEisa_s(Xisa_s, mae).shape\n\n(7, 2, 1, 2)\n\n\n\nsource\n\n\n_strategies\n\n _strategies (mae, xinds:tuple, yinds:tuple, xval:float, yval:float,\n              NrRandom)\n\nCreates strategies (as a particular type of phase space item) for one ax plot point. All strategies have value xval at the xinds index and value yval at the yinds.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmae\n\nMARLDynamics multi-agent environment object\n\n\nxinds\ntuple\nof indices of the phase space item to plot along the x axis\n\n\nyinds\ntuple\nof indices of the phase space item to plot along the y axis\n\n\nxval\nfloat\nthe value of the phase space item to plot along the x axis\n\n\nyval\nfloat\nthe value of the phase space item to plot along the y axis\n\n\nNrRandom\n\nhow many random (in the other dimensions) stratgies for averaging\n\n\nReturns\nndarray\nArray of joint strategies\n\n\n\nFor example, given\n\nenv = SocialDilemma(R=1.0, T=0.8, S=-0.5, P=0.0)\nmae = stratAC(env=env, learning_rates=0.1, discount_factors=0.9)\n\nthe following parameters give\n\nxinds = (0, 0, 0)  # Plot agent 0's state-action item 0, 0\nyinds = (1, 0, 0)  # Plot agent 1's state-action item 0, 0\nNrRandom = 3\n\nstrats = _strategies(mae, xinds, yinds, xval=0.2, yval=0.4, NrRandom=NrRandom)\nassert strats.shape[0] == NrRandom\nstrats.shape\n\n(3, 2, 1, 2)\n\n\nThe first dimension of the _strategies return hold the randomization in the other dimensions than given by xinds and yinds. Note, the randomization in the other dimensions makes no sense in a stateless normal-form game, since there are not other dimensions.\n\nsource\n\n\n_data_to_plot\n\n _data_to_plot (mae, flowarrow_points:Iterable, xinds:tuple, yinds:tuple,\n                NrRandom:int, difffunc:collections.abc.Callable,\n                phasespace_items:collections.abc.Callable, verbose=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmae\n\n\nMARLDynamics multi-agent environment object\n\n\nflowarrow_points\ntyping.Iterable\n\nrange & resolution of flow arrows\n\n\nxinds\ntuple\n\nof indices of the phase space object to plot along the x axis\n\n\nyinds\ntuple\n\nof indices of the phase space object to plot along the y axis\n\n\nNrRandom\nint\n\nhow many random (in the other dimensions) stratgies for averaging\n\n\ndifffunc\nCallable\n\nto compute which kind of arrows to plot (RPE or dX)\n\n\nphasespace_items\nCallable\n\nto obtain phase space items for one ax plot point\n\n\nverbose\nbool\nFalse\nshall I talk to you while working?\n\n\nReturns\ntuple\n\nmeshgrid for (X, Y, dX, dY)\n\n\n\nFor example, given\n\nenv = SocialDilemma(R=1.0, T=0.8, S=-0.5, P=0.0)\nmae = stratAC(env=env, learning_rates=0.1, discount_factors=0.9)\n\nthe following parameters give\n\nxinds = (0, 0, 0)  # Plot agent 0's state-action item 0, 0\nyinds = (1, 0, 0)  # Plot agent 1's state-action item 0, 0\nflowarrow_points = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\nNrRandom=7\ndifffunc = _dRPEisa_s\nphasespace_items = _strategies\nverbose = True\n\nX, Y, dX, dY = _data_to_plot(mae, flowarrow_points, xinds, yinds, NrRandom, difffunc,\n                             phasespace_items=_strategies, verbose=verbose)\n\nassert X.shape == Y.shape; print(\"\\nShape of `X` and `Y`:\", X.shape)\nassert dX.shape == dY.shape; print(\"Shape of `dX` and `dY`:\", dX.shape)\nassert dX.shape[-1] == NrRandom\n\n [plot] generating data 96 %   \nShape of `X` and `Y`: (5, 5)\nShape of `dX` and `dY`: (5, 5, 7)\n\n\nLet \\(l\\) be the number of the flowarrow_points, than X and Y have shape of (\\(l\\), \\(l\\)). dX and dY have shape of (\\(l\\), \\(l\\), Number of randomizations).\n\nsource\n\n\n_plot\n\n _plot (dX:numpy.ndarray, dY:numpy.ndarray, X:numpy.ndarray,\n        Y:numpy.ndarray, ax=None, sf=1.0, col='LEN', cmap='viridis',\n        kind='quiver+samples', lw=1.0, dens=0.75)\n\nPlots the flow for one condition into one axes\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndX\nndarray\n\ndifferences in x dimension\n\n\ndY\nndarray\n\ndifferences in y dimension\n\n\nX\nndarray\n\nmeshgrid in x dimension\n\n\nY\nndarray\n\nmeshgrid in y dimension\n\n\nax\nNoneType\nNone\nIndividual axis to plot into\n\n\nsf\nfloat\n1.0\nScale factor for quiver arrows\n\n\ncol\nstr\nLEN\nwhat should the color indicatie\n\n\ncmap\nstr\nviridis\nColormap\n\n\nkind\nstr\nquiver+samples\nKind of plot: “quiver”, “quiver+samples”, “quiver”, …\n\n\nlw\nfloat\n1.0\nLine width\n\n\ndens\nfloat\n0.75\nDensity\n\n\n\n\nsource\n\n\n_scale\n\n _scale (x:float, y:float, a:float)\n\nScales length of the (x, y) vector accoring to length to the power of a.\n\n\n\n\nType\nDetails\n\n\n\n\nx\nfloat\nx dimension\n\n\ny\nfloat\ny dimension\n\n\na\nfloat\nscaling factor\n\n\nReturns\ntuple\nscaled (x,y)\n\n\n\nA scale factor of 0 makes all vectors equally large.\n\n_scale(4, 3, 0)\n\n(0.8, 0.6000000000000001)\n\n\n\n_scale(40, 30, 0)\n\n(0.8, 0.6)\n\n\nA scale factor of 1 does not change a vector’s length\n\n_scale(4, 3, 1)\n\n(4.0, 3.0)\n\n\n\n_scale(40, 30, 1)\n\n(40.0, 30.0)"
  },
  {
    "objectID": "Utilities/uflowplot.html#examples",
    "href": "Utilities/uflowplot.html#examples",
    "title": "FlowPlot",
    "section": "Examples",
    "text": "Examples\n\nfrom MARLDynamics.Agents.StrategyActorCritic import stratAC\nfrom MARLDynamics.Environments.SocialDilemma import SocialDilemma\n\n\nenv = SocialDilemma(R=1.0, T=0.8, S=-0.5, P=0.0)\nmae = stratAC(env=env, learning_rates=0.1, discount_factors=0.9)\nx = ([0], [0], [0])\ny = ([1], [0], [0])\nflowarrow_points = np.linspace(0.01 ,0.99, 9)\nstandards = [mae, x, y, flowarrow_points]\n\n\nStandard quiver flowplot\nshowing reward-predition error arrows\n\nax = plot_strategy_flow(*standards)\nax[0].set_xlabel(\"Agent 0's cooperation probability\")\nax[0].set_ylabel(\"Agent 1's cooperation probability\");\n\n\n\n\n\n\nStandard quiver flowplot with trajectory\n\nax = plot_strategy_flow(*standards)\nax[0].set_xlabel(\"Agent 0's cooperation probability\")\nax[0].set_ylabel(\"Agent 1's cooperation probability\")\n\nX = mae.random_softmax_strategy()\ntrj, fpr = mae.trajectory(X, Tmax=1000, tolerance=1e-6)\nprint(\"Trajectory length:\", len(trj))\nplot_trajectories([trj], x, y, fprs=[fpr], axes=ax);\n\nTrajectory length: 261\n\n\n\n\n\n\n\nQuiver plot with strategy differences\nNotices how the edges of the phase space differ compared to the plots with reward-prediction errors above.\n\nax = plot_strategy_flow(*standards, use_RPEarrows=False)\nax[0].set_xlabel(\"Agent 0's cooperation probability\")\nax[0].set_ylabel(\"Agent 1's cooperation probability\");\n\n\n\n\n\n\nStreamplot w RPE\nwith reward-prediciton errors\n\nax = plot_strategy_flow(*standards, kind=\"streamplot\")\nax[0].set_xlabel(\"Agent 0's cooperation probability\")\nax[0].set_ylabel(\"Agent 1's cooperation probability\");\nax[0].set_xlim(0, 1); ax[0].set_ylim(0, 1);\n\n\n\n\n\n\nStreamplot w dX\nwith strategy differences\n\nax = plot_strategy_flow(*standards, kind=\"streamplot\", use_RPEarrows=False)\nax[0].set_xlabel(\"Agent 0's cooperation probability\")\nax[0].set_ylabel(\"Agent 1's cooperation probability\");\nax[0].set_xlim(0, 1); ax[0].set_ylim(0, 1);"
  },
  {
    "objectID": "Utilities/uhelpers.html",
    "href": "Utilities/uhelpers.html",
    "title": "Helpers",
    "section": "",
    "text": "source\n\nmake_variable_vector\n\n make_variable_vector (variable, length:int)\n\nTurn a variable into a vector or check that length is consistent.\n\n\n\n\nType\nDetails\n\n\n\n\nvariable\n\ncan be iterable or float or int\n\n\nlength\nint\nlength of the vector\n\n\n\nFor example, when providing a discount factor of 0.9 to all 5 agents, we can simply write\n\nmake_variable_vector(0.9, 5)\n\nDeviceArray([0.9, 0.9, 0.9, 0.9, 0.9], dtype=float32, weak_type=True)\n\n\n\nsource\n\n\ncompute_stationarydistribution\n\n compute_stationarydistribution (Tkk:jax.Array)\n\nCompute stationary distribution for transition matrix Tkk.\n\n\n\n\nType\nDetails\n\n\n\n\nTkk\nArray\nTransition matrix\n\n\n\nFor example, let’s create a random transition matrix with dimension 4:\n\nTkk = np.random.rand(4,4)\n\nA transition matrix contains probabilities, which need to sum up to 1.\n\nTkk = Tkk / Tkk.sum(-1, keepdims=True)\n\ncompute_stationarydistribution should return a 4 by 4 matrix with the stationary distribution in the first column, and the rest filled with a dummy value of -10. This was done to make it work with jax just-in-time-compilation.\n\ncompute_stationarydistribution(Tkk).round(1)\n\nDeviceArray([[  0.4, -10. , -10. , -10. ],\n             [  0.3, -10. , -10. , -10. ],\n             [  0.2, -10. , -10. , -10. ],\n             [  0.1, -10. , -10. , -10. ]], dtype=float32)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MARLDynamics",
    "section": "",
    "text": "clone this repository onto your disk\n\ngit clone https://github.com/wbarfuss/MARLDynamics.git\n\nenter the repositories folder\n\ncd MARLDynamics\n\ninstall locally\n\npip install ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "MARLDynamics",
    "section": "How to use",
    "text": "How to use\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom MARLDynamics.Agents.StrategyActorCritic import stratAC\nfrom MARLDynamics.Environments.SocialDilemma import SocialDilemma\nfrom MARLDynamics.Utils import FlowPlot as fp\n\n# Init enviornment and MultiAgentEnvironment-interface\nenv = SocialDilemma(R=1.0, T=0.8, S=-0.5, P=0.0)\nmae = stratAC(env=env, learning_rates=0.1, discount_factors=0.9)\n\n# Compute learning trajectory \nx = mae.random_softmax_strategy()  # from a random inital policy\nxtraj, fixedpointreached = mae.trajectory(x)\n\n# PLOT\nfig, axs = plt.subplots(1,2, figsize=(9,4))\nplt.subplots_adjust(wspace=0.3)\n\n# Plot in phase space\nx = ([0], [0], [0])\ny = ([1], [0], [0])\nax = fp.plot_strategy_flow(mae, x, y, flowarrow_points = np.linspace(0.01 ,0.99, 9), axes=[axs[0]])\nfp.plot_trajectories([xtraj], x, y, cols=['purple'], axes=ax);\nax[0].set_xlabel(\"Agent 0's cooperation probability\")\nax[0].set_ylabel(\"Agent 1's cooperation probability\");\nax[0].set_title(\"Flowplot\")\n\n# Plot in trajectory\naxs[1].plot(xtraj[:, 0, 0, 0], label=\"Agent 0\", c='red')\naxs[1].plot(xtraj[:, 1, 0, 0], label=\"Agent 1\", c='blue')\naxs[1].set_xlabel('Time steps')\naxs[1].set_ylabel('Cooperation probability')\naxs[1].legend()\naxs[1].set_title(\"Trajectory\");"
  },
  {
    "objectID": "Environments/envhistoryembedding.html",
    "href": "Environments/envhistoryembedding.html",
    "title": "History Embedding",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom MARLDynamics.Environments.SocialDilemma import SocialDilemma\nfrom MARLDynamics.Environments.EcologicalPublicGood import EcologicalPublicGood\n\nfrom MARLDynamics.Agents.StrategyActorCritic import stratAC\nfrom MARLDynamics.Utils import FlowPlot as fp\n\n\nfrom MARLDynamics.Environments.HistoryEmbedding import HistoryEmbedded\n\n\nsocdi = SocialDilemma(R=1.0, T=1.2, S=-0.5, P=0.0)\necopg = EcologicalPublicGood(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.1)\n\n\n\nWith history embedding, we can wrap the standard normal form social dilemma envrionment into one, where the agents condition their action on the actions of the last rounds\n\nmemo1pd = HistoryEmbedded(socdi, h=(1,1,1))\n\nwhich has effectively a state set with the four elements\n\nmemo1pd.Sset\n\n['c,c,.|', 'c,d,.|', 'd,c,.|', 'd,d,.|']\n\n\nAs you can see in the flow plots, this opens the possiblity for cooperation:\n\nmae1 = stratAC(env=memo1pd, learning_rates=0.1, discount_factors=0.9)\nx = ([0], [0,1,2,3], [0])\ny = ([1], [0,1,2,3], [0])\nax = fp.plot_strategy_flow(mae1, x, y, flowarrow_points=np.linspace(0.01 ,0.99, 9), NrRandom=32,\n                           conds=mae1.env.Sset)\n\n\n\n\nIn contrast to the case where agents do not react to the actions of the past round. Here, the only strategy the agents learn is defection:\n\nmae0 = stratAC(env=socdi, learning_rates=0.1, discount_factors=0.9)\nx = ([0], [0], [0])\ny = ([1], [0], [0])\nax = fp.plot_strategy_flow(mae0, x, y, flowarrow_points=np.linspace(0.01 ,0.99, 9), NrRandom=32)\n\n\n\n\nWhat is the effect of having longer action histories?\n\nhlen = 2\nmemoXpd = HistoryEmbedded(socdi, h=(1, hlen, hlen))\nprint( len(memoXpd.Sset) )\n\n16\n\n\n\nmaeX = stratAC(env=memoXpd, learning_rates=0.1, discount_factors=0.9)\n\nfig, ax = plt.subplots(1,1, figsize=(8,9))\nfaps = np.linspace(0.01 ,0.99, 13)\nx = ([0], [0], [0])\ny = ([1], [0], [0])\nfp.plot_strategy_flow(mae1, x, y, flowarrow_points=faps, NrRandom=32, cmap=\"Blues\", axes=[ax])\nfp.plot_strategy_flow(maeX, x, y, flowarrow_points=faps, NrRandom=32, cmap=\"Reds\", axes=[ax]);\nax.set_xlabel(\"Agent 0's cooperation probability\")\nax.set_ylabel(\"Agent 1's cooperation probability\")\nax.set_title(\"Full cooperation in past rounds\");\n\n\n\n\nThe longer action histories give additional force to mutual cooperation when the agents have cooperated in the past rounds and are close to cooperation. This suggests the hypothesis that longer action histories are beneficial for cooperation to be learned. However, more simulation would be needed to answer this question.\n\n\n\nWhat is the effect of condition actions also on the past actions in the ecological public goods envrionment?\n\necopg1 = HistoryEmbedded(ecopg, h=(1,1,1))\necopg1.Sset\n\n['c,c,g|',\n 'c,c,p|',\n 'c,d,g|',\n 'c,d,p|',\n 'd,c,g|',\n 'd,c,p|',\n 'd,d,g|',\n 'd,d,p|']\n\n\nVisualizing the flow of learning in the prosperous state:\n\nmae1 = stratAC(env=ecopg1, learning_rates=0.1, discount_factors=0.9)\nx = ([0], [1,3,5,7], [0])\ny = ([1], [1,3,5,7], [0])\nax = fp.plot_strategy_flow(mae1, x, y, flowarrow_points=np.linspace(0.01 ,0.99, 9), NrRandom=32,\n                           conds=np.array(mae1.env.Sset)[[1,3,5,7]])\n\n\n\n\nThis flow has similarites to the flow of the memory-1 Prisoner’s Dilemma above, yet with more tendency toward cooperation. This is expected, since the ecological public good without memory-1 has also more tendency towards cooperation."
  },
  {
    "objectID": "Environments/envhistoryembedding.html#histories",
    "href": "Environments/envhistoryembedding.html#histories",
    "title": "History Embedding",
    "section": "Histories",
    "text": "Histories\nA history specification determines which realizations of the past the agents will conditions their actions on.\nA history specification h is an iterable of length 1+N. The first value indicates how many time steps of the state observation the agents will use to conditions their actions on. The remaining values indicate how many actions of each agent are used.\n\nsource\n\n_get_all_histories\n\n _get_all_histories (env, h, attr='Z')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nenv\n\n\nAn environment\n\n\nh\n\n\nA history specification\n\n\nattr\nstr\nZ\n\n\n\n\nThe default history specification is h=(1,0,0). Each agent observes information about the previous state, but none about the other agents.\n\n_get_all_histories(socdi, h=(1,0,0))\n\n[('.', '.', 0)]\n\n\n\n_get_all_histories(ecopg, h=(1,0,0))\n\n[('.', '.', 0), ('.', '.', 1)]\n\n\nEach element of these lists is one history. The '.' indicates a dummy value for the non-observable actions. As you can see, here, the actions come before the state information - in contrast to the history specification h. You can think of time traveling from left to right in each history. First the agents choose their joint action, than they observe some state information, after which they choose another joint action. And so on, and so forth.\nFor example, the often used memory-one social dilemmas can be obtained by\n\n_get_all_histories(socdi, h=(0,1,1))\n\n[(0, 0, '.'), (0, 1, '.'), (1, 0, '.'), (1, 1, '.')]\n\n\nHere, the information about the environment is discarded, indicated by the '.'.\nBut the action-history lengths need not be identical,\n\n_get_all_histories(socdi, h=(0,1,2))\n\n[('.', 0, '.', 0, 0, '.'),\n ('.', 0, '.', 0, 1, '.'),\n ('.', 0, '.', 1, 0, '.'),\n ('.', 0, '.', 1, 1, '.'),\n ('.', 1, '.', 0, 0, '.'),\n ('.', 1, '.', 0, 1, '.'),\n ('.', 1, '.', 1, 0, '.'),\n ('.', 1, '.', 1, 1, '.')]\n\n\nHere, each history contains six elements, since it spans two time steps, and each time step is represented by one element for each agent’s action plus one element for the environment.\nOf course, histories can be obtained for any environment.\n\n_get_all_histories(ecopg, h=(2,1,1))\n\n[('.', '.', 0, 0, 0, 0),\n ('.', '.', 0, 0, 0, 1),\n ('.', '.', 0, 0, 1, 0),\n ('.', '.', 0, 0, 1, 1),\n ('.', '.', 0, 1, 0, 0),\n ('.', '.', 0, 1, 0, 1),\n ('.', '.', 0, 1, 1, 0),\n ('.', '.', 0, 1, 1, 1),\n ('.', '.', 1, 0, 0, 0),\n ('.', '.', 1, 0, 0, 1),\n ('.', '.', 1, 0, 1, 0),\n ('.', '.', 1, 0, 1, 1),\n ('.', '.', 1, 1, 0, 0),\n ('.', '.', 1, 1, 0, 1),\n ('.', '.', 1, 1, 1, 0),\n ('.', '.', 1, 1, 1, 1)]\n\n\nWith _get_all_histories we simply iterate through all state and action indicies. However, we are not checking whether a history is actually possible given the transition probabilities of the envrionment.\n\nsource\n\n\n_hist_contains_NotPossibleTrans\n\n _hist_contains_NotPossibleTrans (env, hist:Iterable)\n\nChecks whether the history contains transitions which are not possible with the environment’s transition probabilities.\n\n\n\n\nType\nDetails\n\n\n\n\nenv\n\nAn environment\n\n\nhist\ntyping.Iterable\nA history\n\n\nReturns\nbool\nHistory impossible?\n\n\n\nFor example, in the prosperous state 1 of the ecological public good, when both agents choose the cooperative action 0, there is no chance to leave the propserous state and enter the degraded state 0.\n\n_hist_contains_NotPossibleTrans(ecopg, hist=('.', '.', 1, 0, 0, 0))\n\nTrue\n\n\nThus, any history that contains this transition is not needed.\nYet, if only one agent chooses the defective action 0, a transition to the degraded state becomes possible and corresponding histories cannot be discarded.\n\n_hist_contains_NotPossibleTrans(ecopg, hist=('.', '.', 1, 1, 0, 0))\n\nFalse\n\n\n\n_hist_contains_NotPossibleTrans(ecopg, hist=('.', '.', 1, 0, 1, 0))\n\nFalse\n\n\n\nsource\n\n\nStateActHistsIx\n\n StateActHistsIx (env, h)\n\nReturns all state-action histories (in indices) of env.\nh specifies the type of history. h must be an iterable of length 1+N (where N = Nr. of Agents) The first element of h specifies the length of the state-history Subsequent elements specify the length of the respective action-history\nFor example, the memory-one social dilemmas is obtained by\n\nStateActHistsIx(socdi, h=(0,1,1))\n\n[(0, 0, '.'), (0, 1, '.'), (1, 0, '.'), (1, 1, '.')]\n\n\nwhich is identical to\n\n_get_all_histories(socdi, h=(0,1,1))\n\n[(0, 0, '.'), (0, 1, '.'), (1, 0, '.'), (1, 1, '.')]\n\n\nsince all histories are actually possible in the environment.\nHowever, in our ecological public good example, this is not the case:\n\nlen(StateActHistsIx(ecopg, h=(2,1,1)))\n\n15\n\n\n\nlen(_get_all_histories(ecopg, h=(2,1,1)))\n\n16\n\n\nDepending on the environment, filtering out impossible histories can lead to a significant performance boost.\n\nsource\n\n\nhSset\n\n hSset (env, h)\n\nString representation of the histories.\n\n\n\n\nDetails\n\n\n\n\nenv\nAn environment\n\n\nh\nA history specificaiton\n\n\n\nFor example,\n\nhSset(socdi, h=(1,1,1))\n\n['c,c,.|', 'c,d,.|', 'd,c,.|', 'd,d,.|']\n\n\n\nhSset(ecopg, h=(2,1,1))\n\n[',,g|c,c,g|',\n ',,g|c,c,p|',\n ',,g|c,d,g|',\n ',,g|c,d,p|',\n ',,g|d,c,g|',\n ',,g|d,c,p|',\n ',,g|d,d,g|',\n ',,g|d,d,p|',\n ',,p|c,c,p|',\n ',,p|c,d,g|',\n ',,p|c,d,p|',\n ',,p|d,c,g|',\n ',,p|d,c,p|',\n ',,p|d,d,g|',\n ',,p|d,d,p|']"
  },
  {
    "objectID": "Environments/envhistoryembedding.html#transitions-tensor",
    "href": "Environments/envhistoryembedding.html#transitions-tensor",
    "title": "History Embedding",
    "section": "Transitions tensor",
    "text": "Transitions tensor\n\nsource\n\nhistSjA_TransitionTensor\n\n histSjA_TransitionTensor (env, h)\n\nReturns Transition Tensor of env with state-action history specification h.\nh must be an iterable of length 1+N (where N = Nr. of Agents) The first element of h specifies the length of the state-history Subsequent elements specify the length of the respective action-history\nFor example,\n\nhistSjA_TransitionTensor(socdi, h=(0,1,1)).shape\n\n(4, 2, 2, 4)\n\n\n\nhistSjA_TransitionTensor(ecopg, h=(2,1,1)).shape\n\n(15, 2, 2, 15)"
  },
  {
    "objectID": "Environments/envhistoryembedding.html#reward-tensor",
    "href": "Environments/envhistoryembedding.html#reward-tensor",
    "title": "History Embedding",
    "section": "Reward tensor",
    "text": "Reward tensor\n\nsource\n\nhistSjA_RewardTensor\n\n histSjA_RewardTensor (env, h)\n\nReturns Reward Tensor of env with state-action history specification h.\nh must be an iterable of length 1+N (where N = Nr. of Agents) The first element of h specifies the length of the state-history Subsequent elements specify the length of the respective action-history\nFor example,\n\nhistSjA_RewardTensor(socdi, h=(1,1,1)).shape\n\n(2, 4, 2, 2, 4)\n\n\n\nhistSjA_RewardTensor(ecopg, h=(1,1,1)).shape\n\n(2, 8, 2, 2, 8)"
  },
  {
    "objectID": "Environments/envhistoryembedding.html#partial-observation-and-environmental-uncertainty",
    "href": "Environments/envhistoryembedding.html#partial-observation-and-environmental-uncertainty",
    "title": "History Embedding",
    "section": "Partial observation and environmental uncertainty",
    "text": "Partial observation and environmental uncertainty\nNote: These elements are useful for enviroments with state uncertainty or likewise, partial observability. Such are not yet available in this respository.\n\nsource\n\nObsActHistsIx\n\n ObsActHistsIx (env, h)\n\nReturns all obs-action histories of env.\nh specifies the type of history. h must be an iterable of length 1+N (where N = Nr. of Agents) The first element of h specifies the length of the obs-history Subsequent elements specify the length of the respective action-history\nNote: Here only partial observability regarding the envrionmental state applies. Additional partial observability regarding action is treated seperatly.\n\nObsActHistsIx(socdi, h=(1,1,1))\n\n[(0, 0, 0), (0, 1, 0), (1, 0, 0), (1, 1, 0)]\n\n\n\nsource\n\n\nhOset\n\n hOset (env, h)\n\n\nhOset(socdi, h=(1,1,1))\n\n[['c,c,.|', 'c,d,.|', 'd,c,.|', 'd,d,.|'],\n ['c,c,.|', 'c,d,.|', 'd,c,.|', 'd,d,.|']]\n\n\n\nsource\n\n\nhistSjA_ObservationTensor\n\n histSjA_ObservationTensor (env, h)\n\nReturns Observation Tensor of env with state-action history h[iterable]\n\nhistSjA_ObservationTensor(socdi, h=(1,1,1))\n\narray([[[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.]],\n\n       [[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.]]])"
  },
  {
    "objectID": "Environments/envhistoryembedding.html#environment-wrapper",
    "href": "Environments/envhistoryembedding.html#environment-wrapper",
    "title": "History Embedding",
    "section": "Environment wrapper",
    "text": "Environment wrapper\n\nsource\n\nHistoryEmbedded\n\n HistoryEmbedded (env, h)\n\nAbstract Environment wrapper to embed a given environment into a larger history space\nh must be an iterable of length 1+N (where N=Nr. of Agents) The first element of history specifies the length of the state-history. Subsequent elements specify the length of the respective action-history\n\n\n\n\nDetails\n\n\n\n\nenv\nAn environment\n\n\nh\nHistory specification"
  },
  {
    "objectID": "Environments/envbase.html",
    "href": "Environments/envbase.html",
    "title": "Environment Base",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "Environments/envbase.html#core-methods",
    "href": "Environments/envbase.html#core-methods",
    "title": "Environment Base",
    "section": "Core methods",
    "text": "Core methods\nThese need to be implemented by a concrete environment.\nThe transitions tensor Tsjas' gives the probability of the environment to transition to state s', given that it was in state s and the agent chose the joint action ja.\n\nsource\n\nebase.TransitionTensor\n\n ebase.TransitionTensor ()\n\n\nclass slf: pass\ntest_fail(ebase.TransitionTensor, args=slf)\n\nraises NotImplementedError.\nThe reward tensor Risjas' gives the reward agent i receives when the environment is in state s, all agents choose the join action ja, and the environment transitions to state s'.\n\nsource\n\n\nebase.RewardTensor\n\n ebase.RewardTensor ()\n\n\nclass slf: pass\ntest_fail(ebase.RewardTensor, args=slf)\n\nraises NotImplementedError.\nThe following two “core” methods are optional. If the concrete environment class does not implement them, they default to the following:\nThe observation tensor Oiso gives the probability that agent i observes observation o when the environment is in state s. The default observation tensor assumes perfect observation and sets the number of observations Q to the number of states Z.\n\nsource\n\n\nebase.ObservationTensor\n\n ebase.ObservationTensor ()\n\nDefault observation tensor: perfect observation\n\nclass slf: Z = 2; N = 3  # dummy self for demonstration only\nebase.ObservationTensor(slf)\n\narray([[[1., 0.],\n        [0., 1.]],\n\n       [[1., 0.],\n        [0., 1.]],\n\n       [[1., 0.],\n        [0., 1.]]])\n\n\nFinal states Fs indicate which states of the environment cause the end of an episode. Their meaning and use within MARLDynamics are not fully resolved yet. If an environment does not implement FinalStates they default to no final states.\n\nsource\n\n\nebase.FinalStates\n\n ebase.FinalStates ()\n\nDefault final states: no final states\n\nclass slf: Z = 7 # dummy self for demonstration only\nebase.FinalStates(slf)\n\narray([0, 0, 0, 0, 0, 0, 0])"
  },
  {
    "objectID": "Environments/envbase.html#default-string-representations",
    "href": "Environments/envbase.html#default-string-representations",
    "title": "Environment Base",
    "section": "Default string representations",
    "text": "Default string representations\nString representations of actions, states and observations help with interpreting the results of simulation runs. Ideally, an environment class will implement these methods with descriptive values.\nTo show these methods here we create a dummy “self” of 2 environmental states, containing 3 agents with 4 actions and 5 observations of the environmental states.\n\n# dummy self of 2 environmental 2 agents with 3 actions in an environment\nclass slf: Z = 2; N = 3; M=4; Q=5\n\n\nsource\n\nebase.actions\n\n ebase.actions ()\n\nDefault action set representations act_im.\n\nebase.actions(slf)\n\n[['0', '1', '2', '3'], ['0', '1', '2', '3'], ['0', '1', '2', '3']]\n\n\n\nsource\n\n\nebase.states\n\n ebase.states ()\n\nDefault state set representation state_s.\n\nebase.states(slf)\n\n['0', '1']\n\n\n\nsource\n\n\nebase.observations\n\n ebase.observations ()\n\nDefault observation set representations obs_io.\n\nebase.observations(slf)\n\n[['0', '1', '2', '3', '4'],\n ['0', '1', '2', '3', '4'],\n ['0', '1', '2', '3', '4']]\n\n\n\nsource\n\n\nebase.id\n\n ebase.id ()\n\nReturns id string of environment"
  },
  {
    "objectID": "Environments/envbase.html#interactive-use",
    "href": "Environments/envbase.html#interactive-use",
    "title": "Environment Base",
    "section": "Interactive use",
    "text": "Interactive use\nEnvironments can also be used interactivly, e.g., with iterative learning algorithms. For this purpose we provide the OpenAI Gym step Interface.\n\nsource\n\nebase.step\n\n ebase.step (jA:Iterable)\n\nIterate the environment one step forward.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\njA\ntyping.Iterable\njoint actions\n\n\nReturns\ntuple\n(observations_i, rewards_i, done, info)\n\n\n\n\nsource\n\n\nebase.observation\n\n ebase.observation ()\n\nPossibly random observation for each agent from the current state."
  },
  {
    "objectID": "Environments/envecologicalpublicgood.html",
    "href": "Environments/envecologicalpublicgood.html",
    "title": "Ecological Public Good",
    "section": "",
    "text": "The environment was introduced in"
  },
  {
    "objectID": "Environments/envecologicalpublicgood.html#example",
    "href": "Environments/envecologicalpublicgood.html#example",
    "title": "Ecological Public Good",
    "section": "Example",
    "text": "Example\n\nfrom MARLDynamics.Environments.EcologicalPublicGood import EcologicalPublicGood\nfrom MARLDynamics.Agents.StrategyActorCritic import stratAC\nfrom MARLDynamics.Utils import FlowPlot as fp\nimport numpy as np\n\n\nenv = EcologicalPublicGood(N=2, f=1.2, c=5, m=-4, qc=0.2, qr=0.1, degraded_choice=True)\nenv\n\nEcologicalPublicGood_2_1.2_5_-4_0.2_0.1_DegChoi\n\n\nIn the prosperous state, the rewards are a tragedy Prisoners’ Dilemma.\n\nenv.R[0,1,:,:,1], env.R[1,1,:,:,1]\n\n(array([[ 1., -2.],\n        [ 3.,  0.]]),\n array([[ 1.,  3.],\n        [-2.,  0.]]))\n\n\nYet, because of the possible collapse and the agents’ future outlook, the overall regime is one of coordination.\n\n# Init enviornment and MultiAgentEnvironment-interface\nmae = stratAC(env=env, learning_rates=0.1, discount_factors=0.9)\n\nx = ([0], [0,1], [0])  # Plotting only the prosperous state\ny = ([1], [0,1], [0])  # Plotting only the prosperous state\nax = fp.plot_strategy_flow(mae, x, y, flowarrow_points = np.linspace(0.01 ,0.99, 9), NrRandom=16)"
  },
  {
    "objectID": "Environments/envecologicalpublicgood.html#implementation",
    "href": "Environments/envecologicalpublicgood.html#implementation",
    "title": "Ecological Public Good",
    "section": "Implementation",
    "text": "Implementation\n\nsource\n\nEcologicalPublicGood\n\n EcologicalPublicGood (N:int, f:Union[float,Iterable],\n                       c:Union[float,Iterable], m:Union[float,Iterable],\n                       qc:Union[float,Iterable], qr:Union[float,Iterable],\n                       degraded_choice=False)\n\nEcological Public Good Environment.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n\nnumber of agents\n\n\nf\ntyping.Union[float, typing.Iterable]\n\npublic goods synergy factor\n\n\nc\ntyping.Union[float, typing.Iterable]\n\ncost of cooperation\n\n\nm\ntyping.Union[float, typing.Iterable]\n\ncollapse impact\n\n\nqc\ntyping.Union[float, typing.Iterable]\n\ncollapse leverage/timescale\n\n\nqr\ntyping.Union[float, typing.Iterable]\n\nrecovery leverage/timescale\n\n\ndegraded_choice\nbool\nFalse\nwhether agents have a choice at the degraded state\n\n\n\n\nsource\n\n\nEcologicalPublicGood.actions\n\n EcologicalPublicGood.actions ()\n\nThe action sets\n\nsource\n\n\nEcologicalPublicGood.states\n\n EcologicalPublicGood.states ()\n\nThe states set\n\nsource\n\n\nEcologicalPublicGood.TransitionTensor\n\n EcologicalPublicGood.TransitionTensor ()\n\nGet the Transition Tensor.\nThe TransitionTensor is obtained with the help of the _transition_probability method.\n\nsource\n\n\nEcologicalPublicGood._transition_probability\n\n EcologicalPublicGood._transition_probability (s:int, jA:Iterable, s_:int)\n\nReturns the transition probability for current state s, joint action jA, and next state s_.\n\n\n\n\nType\nDetails\n\n\n\n\ns\nint\nthe state index\n\n\njA\ntyping.Iterable\nindices for joint actions\n\n\ns_\nint\nthe next-state index\n\n\nReturns\nfloat\ntransition probability\n\n\n\n\nsource\n\n\nEcologicalPublicGood.RewardTensor\n\n EcologicalPublicGood.RewardTensor ()\n\nGet the Reward Tensor R[i,s,a1,…,aN,s’].\nThe RewardTensor is obtained with the help of the _reward method.\n\nsource\n\n\nEcologicalPublicGood._reward\n\n EcologicalPublicGood._reward (i:int, s:int, jA:Iterable, s_:int)\n\nReturns the reward value for agent i in current state s, under joint action jA, when transitioning to next state s_.\n\n\n\n\nType\nDetails\n\n\n\n\ni\nint\nthe agent index\n\n\ns\nint\nthe state index\n\n\njA\ntyping.Iterable\nindices for joint actions\n\n\ns_\nint\nthe next-state index\n\n\nReturns\nfloat\nreward value\n\n\n\n\nsource\n\n\nEcologicalPublicGood.id\n\n EcologicalPublicGood.id ()\n\nReturns id string of environment"
  },
  {
    "objectID": "Environments/envsocialdilemma.html",
    "href": "Environments/envsocialdilemma.html",
    "title": "Social Dilemma",
    "section": "",
    "text": "Typical examples are the Prisoner’s Dilemma, Stag Hunt game, and the game of chicken/snowdrift/hawk-dove.\n\nsource\n\nSocialDilemma\n\n SocialDilemma (R:float, T:float, S:float, P:float)\n\nSymmetric 2-agent 2-action Social Dilemma Matrix Game.\n\n\n\n\nType\nDetails\n\n\n\n\nR\nfloat\nreward of mutual cooperation\n\n\nT\nfloat\ntemptation of unilateral defection\n\n\nS\nfloat\nsucker’s payoff of unilateral cooperation\n\n\nP\nfloat\npunsihment of mutual defection\n\n\n\n\nsource\n\n\nSocialDilemma.TransitionTensor\n\n SocialDilemma.TransitionTensor ()\n\nGet the Transition Tensor.\n\nsource\n\n\nSocialDilemma.RewardTensor\n\n SocialDilemma.RewardTensor ()\n\nGet the Reward Tensor R[i,s,a1,…,aN,s’].\n\nsource\n\n\nSocialDilemma.actions\n\n SocialDilemma.actions ()\n\nThe action sets\n\nsource\n\n\nSocialDilemma.states\n\n SocialDilemma.states ()\n\nThe states set\n\nsource\n\n\nSocialDilemma.id\n\n SocialDilemma.id ()\n\nReturns id string of environment\n\n\nExample\n\nenv = SocialDilemma(R=1, T=2, S=-1, P=0)\n\n\nenv.id()\n\n'SocialDilemma_2_1_0_-1'\n\n\n\nenv\n\nSocialDilemma_2_1_0_-1\n\n\nReward matrix of agent 0:\n\nenv.RewardTensor()[0,0,:,:,0]\n\narray([[ 1., -1.],\n       [ 2.,  0.]])\n\n\nReward matrix of agent 1:\n\nenv.RewardTensor()[1,0,:,:,0]\n\narray([[ 1.,  2.],\n       [-1.,  0.]])\n\n\n\nenv.TransitionTensor()\n\narray([[[[1.],\n         [1.]],\n\n        [[1.],\n         [1.]]]])\n\n\n\nenv.actions()\n\n[['c', 'd'], ['c', 'd']]\n\n\n\nenv.states()\n\n['.']"
  },
  {
    "objectID": "Agents/abase.html",
    "href": "Agents/abase.html",
    "title": "Base",
    "section": "",
    "text": "contains core methods to compute the strategy-average reward-prediction error.\n\nsource\n\n\n\n abase (TransitionTensor:numpy.ndarray, RewardTensor:numpy.ndarray,\n        DiscountFactors:Iterable[float], use_prefactor=False,\n        opteinsum=True)\n\nBase class for deterministic strategy-average independent (multi-agent) temporal-difference reinforcement learning.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nTransitionTensor\nndarray\n\ntransition model of the environment\n\n\nRewardTensor\nndarray\n\nreward model of the environment\n\n\nDiscountFactors\ntyping.Iterable[float]\n\nthe agents’ discount factors\n\n\nuse_prefactor\nbool\nFalse\nuse the 1-DiscountFactor prefactor\n\n\nopteinsum\nbool\nTrue\noptimize einsum functions"
  },
  {
    "objectID": "Agents/abase.html#strategy-averaging",
    "href": "Agents/abase.html#strategy-averaging",
    "title": "Base",
    "section": "Strategy averaging",
    "text": "Strategy averaging\nCore methods to compute the strategy-average reward-prediction error\n\nsource\n\nabase.Tss\n\n abase.Tss (Xisa:jax.Array)\n\nCompute average transition model Tss, given joint strategy Xisa\n\n\n\n\nType\nDetails\n\n\n\n\nXisa\nArray\nJoint strategy\n\n\nReturns\nArray\nAverage transition matrix\n\n\n\n\nsource\n\n\nabase.Tisas\n\n abase.Tisas (Xisa:jax.Array)\n\nCompute average transition model Tisas, given joint strategy Xisa\n\n\n\n\nType\nDetails\n\n\n\n\nXisa\nArray\nJoint strategy\n\n\nReturns\nArray\nAverage transition Tisas\n\n\n\n\nsource\n\n\nabase.Ris\n\n abase.Ris (Xisa:jax.Array, Risa:jax.Array=None)\n\nCompute average reward Ris, given joint strategy Xisa\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nXisa\nArray\n\nJoint strategy\n\n\nRisa\nArray\nNone\nOptional reward for speed-up\n\n\nReturns\nArray\n\nAverage reward\n\n\n\n\nsource\n\n\nabase.Risa\n\n abase.Risa (Xisa:jax.Array)\n\nCompute average reward Risa, given joint strategy Xisa\n\n\n\n\nType\nDetails\n\n\n\n\nXisa\nArray\nJoint strategy\n\n\nReturns\nArray\nAverage reward\n\n\n\n\nsource\n\n\nabase.Vis\n\n abase.Vis (Xisa:jax.Array, Ris:jax.Array=None, Tss:jax.Array=None,\n            Risa:jax.Array=None)\n\nCompute average state values Vis, given joint strategy Xisa\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nXisa\nArray\n\nJoint strategy\n\n\nRis\nArray\nNone\nOptional reward for speed-up\n\n\nTss\nArray\nNone\nOptional transition for speed-up\n\n\nRisa\nArray\nNone\nOptional reward for speed-up\n\n\nReturns\nArray\n\nAverage state values\n\n\n\n\nsource\n\n\nabase.Qisa\n\n abase.Qisa (Xisa:jax.Array, Risa:jax.Array=None, Vis:jax.Array=None,\n             Tisas:jax.Array=None)\n\nCompute average state-action values Qisa, given joint strategy Xisa\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nXisa\nArray\n\nJoint strategy\n\n\nRisa\nArray\nNone\nOptional reward for speed-up\n\n\nVis\nArray\nNone\nOptional values for speed-up\n\n\nTisas\nArray\nNone\nOptional transition for speed-up\n\n\nReturns\nArray\n\nAverage state-action values"
  },
  {
    "objectID": "Agents/abase.html#helpers",
    "href": "Agents/abase.html#helpers",
    "title": "Base",
    "section": "Helpers",
    "text": "Helpers\n\nsource\n\nabase.Ps\n\n abase.Ps (Xisa:jax.Array)\n\nCompute stationary state distribution Ps, given joint strategy Xisa.\n\n\n\n\nType\nDetails\n\n\n\n\nXisa\nArray\nJoint strategy\n\n\nReturns\nArray\nStationary state distribution\n\n\n\nPs uses the compute_stationarydistribution function.\n\nfrom MARLDynamics.Environments.EcologicalPublicGood import EcologicalPublicGood as EPG\nfrom MARLDynamics.Agents.StrategyActorCritic import stratAC\n\n\nenv = EPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01, degraded_choice=False)\nMAEi = stratAC(env=env, learning_rates=0.1, discount_factors=0.99, use_prefactor=True)\n\nx = MAEi.random_softmax_strategy()\nMAEi._numpyPs(x)\n\narray([0.9284509, 0.0715491], dtype=float32)\n\n\n\nMAEi.Ps(x)\n\nDeviceArray([0.9284509, 0.0715491], dtype=float32)\n\n\n\nsource\n\n\nabase.Ri\n\n abase.Ri (Xisa:jax.Array)\n\nCompute average reward Ri, given joint strategy Xisa.\n\n\n\n\nType\nDetails\n\n\n\n\nXisa\nArray\nJoint strategy Xisa\n\n\nReturns\nArray\nAverage reward Ri\n\n\n\n\nMAEi.Ri(x)\n\nDeviceArray([-4.753777, -4.577655], dtype=float32)\n\n\n\nsource\n\n\nabase.trajectory\n\n abase.trajectory (Xinit:jax.Array, Tmax:int=100, tolerance:float=None,\n                   verbose=False, **kwargs)\n\nCompute a joint learning trajectory.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nXinit\nArray\n\nInitial condition\n\n\nTmax\nint\n100\nthe maximum number of iteration steps\n\n\ntolerance\nfloat\nNone\nto determine if a fix point is reached\n\n\nverbose\nbool\nFalse\nSay something during computation?\n\n\nkwargs\n\n\n\n\n\nReturns\ntuple\n\n(trajectory, fixpointreached)\n\n\n\ntrajectory is an Array containing the time-evolution of the dynamic variable. fixpointreached is a bool saying whether or not a fixed point has been reached.\n\nsource\n\n\nabase._OtherAgentsActionsSummationTensor\n\n abase._OtherAgentsActionsSummationTensor ()\n\nTo sum over the other agents and their respective actions using einsum.\nTo obtain the strategy-average reward-prediction error for agent \\(i\\), we need to average out the probabilities contained in the strategies of all other agents \\(j \\neq i\\) and the transition function \\(T\\),\n\\[\n\\sum_{a^j} \\sum_{s'} \\prod_{i\\neq j} X^j(s, a^j) T(s, \\mathbf a, s').\n\\]\nThe _OtherAgentsActionsSummationTensor enables this summation to be exectued in the efficient einsum function. It contains only \\(0\\)s and \\(1\\)s and is of dimension\n\\[\nN \\times \\underbrace{N \\times ... \\times N}_{(N-1) \\text{ times}}\n\\times M \\times \\underbrace{M \\times ... \\times M}_{N \\text{ times}}\n\\times \\underbrace{M \\times ... \\times M}_{(N-1) \\text{ times}}\n\\]\nwhich represent\n\\[\n\\overbrace{N}^{\\text{the focal agent}}\n\\times\n\\overbrace{\\underbrace{N \\times ... \\times N}_{(N-1) \\text{ times}}}^\\text{all other agents}\n\\times\n\\overbrace{M}^\\text{focal agent's action}\n\\times\n\\overbrace{\\underbrace{M \\times ... \\times M}_{N \\text{ times}}}^\\text{all actions}\n\\times\n\\overbrace{\\underbrace{M \\times ... \\times M}_{(N-1) \\text{ times}}}^\\text{all other agents' actions}\n\\]\nIt contains a \\(1\\) only if\n\nall agent indices (comprised of the focal agent index and all other agents indices) are different from each other\nand the focal agent’s action index matches the focal agents’ action index in all actions\nand if all other agents’ action indices match their corresponding action indices in all actions.\n\nOtherwise it contains a \\(0\\)."
  },
  {
    "objectID": "Agents/astrategysarsa.html",
    "href": "Agents/astrategysarsa.html",
    "title": "Strategy SARSA",
    "section": "",
    "text": "SARSA agents take into acount the five pieces of information of current State, current Action, Reward, next State and next Action."
  },
  {
    "objectID": "Agents/astrategysarsa.html#example",
    "href": "Agents/astrategysarsa.html#example",
    "title": "Strategy SARSA",
    "section": "Example",
    "text": "Example\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom MARLDynamics.Environments.SocialDilemma import SocialDilemma\nfrom MARLDynamics.Utils import FlowPlot as fp\n\nfrom MARLDynamics.Agents.StrategyActorCritic import stratAC\nfrom MARLDynamics.Agents.StrategySARSA import stratSARSA\n\n\nenv = SocialDilemma(R=1.0, T=0.8, S=-0.5, P=0.0)\n\nLet’s compare the SARSA (in red) with the actor-critic learners (in blue). The difference is that the SARSA learners incorperate an explicit exploration term in their learning update, regulated by the choice_intensities. For low choice intensities, the SARSA learners tend to extreme exploration, i.e., toward the center of the strategy space. For high choice intensities, the SARSA map onto the actor-critic learners (see Figure below). For the actor-critic learners, the choice_intensities have not effect other than scaling the speed of learning alongside the learning rates.\n\nfig, ax = plt.subplots(1,4, figsize=(18,4))\nfaps = np.linspace(0.01 ,0.99, 9)\nx = ([0], [0], [0])\ny = ([1], [0], [0])\n\nfor i, ci in enumerate([0.1, 1.0, 10, 100]):\n\n    maeAC = stratAC(env=env, learning_rates=0.1, discount_factors=0.9, choice_intensities=ci)\n    maeSARSA = stratSARSA(env=env, learning_rates=0.1, discount_factors=0.9, choice_intensities=ci)\n\n    fp.plot_strategy_flow(maeAC, x, y, flowarrow_points=faps, cmap=\"Blues\", axes=[ax[i]])\n    fp.plot_strategy_flow(maeSARSA, x, y, flowarrow_points=faps, cmap=\"Reds\", axes=[ax[i]]);\n\n    ax[i].set_xlabel(\"Agent 0's cooperation probability\")\n    ax[i].set_ylabel(\"Agent 1's cooperation probability\")\n    ax[i].set_title(\"Intensity of choice {}\".format(ci));"
  },
  {
    "objectID": "Agents/astrategysarsa.html#api",
    "href": "Agents/astrategysarsa.html#api",
    "title": "Strategy SARSA",
    "section": "API",
    "text": "API\n\nsource\n\nstratSARSA\n\n stratSARSA (env, learning_rates:Union[float,Iterable],\n             discount_factors:Union[float,Iterable],\n             choice_intensities:Union[float,Iterable]=1.0,\n             use_prefactor=False, opteinsum=True, **kwargs)\n\nClass for deterministic strategy-average independent (multi-agent) temporal-difference SARSA reinforcement learning in strategy space.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nenv\n\n\nAn environment object\n\n\nlearning_rates\ntyping.Union[float, typing.Iterable]\n\nagents’ learning rates\n\n\ndiscount_factors\ntyping.Union[float, typing.Iterable]\n\nagents’ discount factors\n\n\nchoice_intensities\ntyping.Union[float, typing.Iterable]\n1.0\nagents’ choice intensities\n\n\nuse_prefactor\nbool\nFalse\nuse the 1-DiscountFactor prefactor\n\n\nopteinsum\nbool\nTrue\noptimize einsum functions\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nstratSARSA.RPEisa\n\n stratSARSA.RPEisa (Xisa, norm=False)\n\nCompute reward-prediction/temporal-difference error for strategy SARSA dynamics, given joint strategy Xisa.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nXisa\n\n\nJoint strategy\n\n\nnorm\nbool\nFalse\nnormalize error around actions?\n\n\nReturns\nndarray\n\nRP/TD error\n\n\n\n\nsource\n\n\nstratSARSA.NextQisa\n\n stratSARSA.NextQisa (Xisa, Qisa=None, Risa=None, Vis=None, Tisas=None)\n\nCompute strategy-average next state-action value for agent i, current state s and action a.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nXisa\n\n\nJoint strategy\n\n\nQisa\nNoneType\nNone\nOptional state-action values for speed-up\n\n\nRisa\nNoneType\nNone\nOptional rewards for speed-up\n\n\nVis\nNoneType\nNone\nOptional state values for speed-up\n\n\nTisas\nNoneType\nNone\nOptional transition for speed-up\n\n\nReturns\nArray\n\nNext values\n\n\n\nNote, that although maeSARSA.NextQisa is computed differently than maeAC.NextVisa, they give actually identical values.\n\nci = 100 * np.random.rand()\n\nmaeAC = stratAC(env=env, learning_rates=0.1, discount_factors=0.9, choice_intensities=ci)\nmaeSARSA = stratSARSA(env=env, learning_rates=0.1, discount_factors=0.9, choice_intensities=ci)\n\nX = maeAC.random_softmax_strategy()\n\nassert np.allclose(maeAC.NextVisa(X) - maeSARSA.NextQisa(X), 0, atol=1e-05)"
  },
  {
    "objectID": "Agents/astrategybase.html",
    "href": "Agents/astrategybase.html",
    "title": "Strategy Base",
    "section": "",
    "text": "source\n\nstrategybase\n\n strategybase (env, learning_rates:Union[float,Iterable],\n               discount_factors:Union[float,Iterable],\n               choice_intensities:Union[float,Iterable]=1.0,\n               use_prefactor=False, opteinsum=True, **kwargs)\n\nBase class for deterministic strategy-average independent (multi-agent) temporal-difference reinforcement learning in strategy space.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nenv\n\n\nAn environment object\n\n\nlearning_rates\ntyping.Union[float, typing.Iterable]\n\nagents’ learning rates\n\n\ndiscount_factors\ntyping.Union[float, typing.Iterable]\n\nagents’ discount factors\n\n\nchoice_intensities\ntyping.Union[float, typing.Iterable]\n1.0\nagents’ choice intensities\n\n\nuse_prefactor\nbool\nFalse\nuse the 1-DiscountFactor prefactor\n\n\nopteinsum\nbool\nTrue\noptimize einsum functions\n\n\nkwargs\n\n\n\n\n\n\nFurther optional paramerater inherting from abase:\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nuse_prefactor\nbool\nFalse\nuse the 1-DiscountFactor prefactor\n\n\nopteinsum\nbool\nTrue\noptimize einsum functions\n\n\n\n\nsource\n\n\nstrategybase.step\n\n strategybase.step (Xisa)\n\nPerforms a learning step along the reward-prediction/temporal-difference error in strategy space, given joint strategy Xisa.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nXisa\n\nJoint strategy\n\n\nReturns\ntuple\n(Updated joint strategy, Prediction error)\n\n\n\n\nsource\n\n\nstrategybase.zero_intelligence_strategy\n\n strategybase.zero_intelligence_strategy ()\n\nReturns strategy Xisa with equal action probabilities.\n\nsource\n\n\nstrategybase.random_softmax_strategy\n\n strategybase.random_softmax_strategy ()\n\nReturns softmax strategy Xisa with random action probabilities.\n\nsource\n\n\nstrategybase.id\n\n strategybase.id ()\n\nReturns an identifier to handle simulation runs."
  },
  {
    "objectID": "Agents/astrategyactorcritic.html",
    "href": "Agents/astrategyactorcritic.html",
    "title": "Strategy Actor-Critic",
    "section": "",
    "text": "source\n\nstratAC\n\n stratAC (env, learning_rates:Union[float,Iterable],\n          discount_factors:Union[float,Iterable],\n          choice_intensities:Union[float,Iterable]=1.0,\n          use_prefactor=False, opteinsum=True, **kwargs)\n\nClass for deterministic policy-average independent (multi-agent) temporal-difference actor-critic reinforcement learning in strategy space.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nenv\n\n\nAn environment object\n\n\nlearning_rates\ntyping.Union[float, typing.Iterable]\n\nagents’ learning rates\n\n\ndiscount_factors\ntyping.Union[float, typing.Iterable]\n\nagents’ discount factors\n\n\nchoice_intensities\ntyping.Union[float, typing.Iterable]\n1.0\nagents’ choice intensities\n\n\nuse_prefactor\nbool\nFalse\nuse the 1-DiscountFactor prefactor\n\n\nopteinsum\nbool\nTrue\noptimize einsum functions\n\n\nkwargs\n\n\n\n\n\n\nNote, choice_intensities are not required for actor-critic learning and have no other effect than scaling the learning_rates. Hence the default value of 1.\n\nsource\n\n\nstratAC.RPEisa\n\n stratAC.RPEisa (Xisa, norm=False)\n\nCompute reward-prediction/temporal-difference error for strategy actor-critic dynamics, given joint strategy Xisa.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nXisa\n\n\nJoint strategy\n\n\nnorm\nbool\nFalse\nnormalize error around actions?\n\n\nReturns\nndarray\n\nRP/TD error\n\n\n\n\nsource\n\n\nstratAC.NextVisa\n\n stratAC.NextVisa (Xisa, Vis=None, Tss=None, Ris=None, Risa=None)\n\nCompute strategy-average next value for agent i, current state s and action a.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nXisa\n\n\nJoint strategy\n\n\nVis\nNoneType\nNone\nOptional values for speed-up\n\n\nTss\nNoneType\nNone\nOptional transition for speed-up\n\n\nRis\nNoneType\nNone\nOptional reward for speed-up\n\n\nRisa\nNoneType\nNone\nOptional reward for speed-up\n\n\nReturns\nArray\n\nNext values"
  }
]